\subsection{Discussion}
\subsubsection{Balancing consistency and tracking error}
When tuning we want to achieve as good tracking as possible, but we also want the filter to be consistent. An overly confident filter will not fuse measurments that conflict with the prediction (i.e loosing track), while an uncertian filter will struggle to aquire a track at all. The filter will not always have perfect track, but it is important that the "confidence" of the filter scales well with how good the track actually is.
When tuning we therefore use the NEES CI as a metric for how confident the filter is compared to how well the filter tracks the target. It should be noted that calculating NEES for a single dataset is not ideal, and we should run several iterations (Monte Carlo simulations) of this to test how well the filter generalizes.

\subsubsection{IMM-PDAF vs simpler models} \label{whyimmpdaf}
We tried the much simpler CV and CT PDAF models to solve the tracking problem and both worked suprisingly well on the Joyride dataset. They were much simpler to tune and we even got better consistency than the IMM-PDAF, but with a small increase in tracking error (RMSE). For simple gaussian linear single component models, such as the CV or CT model, the prediction is just a gaussian blob and it is easy to scale the noise input to improve consistency. The IMM-PDAF is obviously more challenging to get consisent due to the mixture of interconnected models. However, it does provide the benefit of better tracking accuracy since it allows the use of specialized models for each scenario which increases our ability to do correct predictions. Intuitively we want to avoid sharing the probability mass too much between multiple models, but rather try to select one filter (one mode) at a time.
